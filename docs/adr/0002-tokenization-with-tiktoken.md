# ADR-0002: Token カウントに tiktoken を採用する

## Context
- 背景: Persona 生成や context trimming では会話のトークン数を正確に把握する必要がある。
- 課題: 既存実装は `len(text) / 4` の概算フォールバックを行っており、モデル上限付近で誤差が大きい。
- 参考: OpenAI 公式トークナイザ `tiktoken` のドキュメント、および社内評価ログ。

## Decision
- 方針: Python ランタイムに `tiktoken>=0.7.0` を必須依存関係として追加し、`core_ext/context_trimmer.py` 等の計測ロジックを tiktoken ベースに置き換える。
- 決定理由: OpenAI および互換モデル向けに最適化された公式トークナイザを利用することで、推論コストやトークン上限を厳密に制御できる。
- 適用範囲: 非対応モデル（例: Gemini）の場合は互換エンコーディングが出揃うまで従来の概算ロジックを残し、プロバイダ単位に切り替える。

## Consequences
- 影響範囲: OpenAI 系モデルでのトークン上限制御の精度が向上し、不要なコンテキスト切り捨てやエラー再試行が減る。
- コスト: 追加依存によりデプロイイメージが僅かに肥大化するが、CPU 利用とレイテンシへの影響は 5% 未満に収まる見込み。
- リスク/フォローアップ: Gemini 等の将来的なマルチプロバイダ対応では、互換トークナイザの選定が継続課題となる。

## Status
- ステータス: 承認済み
- 最終更新日: 2025-02-14

## DoD
- `requirements.txt` / `requirements-eval.txt` に tiktoken の固定バージョンを追加している。
- トークンカウントを行うユニットテストが tiktoken を利用した実測値に基づき緑化している。
- Gemini 等非対応プロバイダ向けフォールバック実装の仕様をドキュメント化している。
